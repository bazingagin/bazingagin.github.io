<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>Zhiying (Gin)  Jiang | publications</title>
<meta name="description" content="gin's personal website
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->
<link rel="shortcut icon" href="/assets/img/favicon.ico">
<link rel="stylesheet" href="/assets/css/main.css">

<link rel="canonical" href="/publications/">
<link rel="preconnect" href="https://fonts.gstatic.com">
<link href="https://fonts.googleapis.com/css2?family=DM+Sans&display=swap" rel="stylesheet">

<!-- Theming-->

  <script src="/assets/js/theme.js"></script>




    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav sticky-bottom-footer">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="http://localhost:4000/">
       <span class="font-weight-bold">Zhiying (Gin)   Jiang</span>
      </a>
      
      <!-- Navbar Toogle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              about
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item active">
              <a class="nav-link" href="/publications/">
                publications
                
                <span class="sr-only">(current)</span>
                
              </a>
          </li>
          
          
          
            <div class = "toggle-container">
              <a id = "light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    <h1 class="post-title">publications</h1>
    <p class="post-description">publications in reversed chronological order.</p>
  </header>

  <article>
    <div class="publications">


  <h2 class="year">2022</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">NeurIPS2022</abbr>
    
  
  </div>

  <div id="jiang2022few" class="col-sm-8">
    
      <div class="title">Few-Shot Non-Parametric Learning with Deep Latent Variable Model</div>
      <div class="author">
        
          
          
          
            
              
                
                  Jiang, Zhiying,
                
              
            
          
        
          
          
          
            
              
                
                  Dai, Yiqin,
                
              
            
          
        
          
          
          
            
              
                
                  Xin, Ji,
                
              
            
          
        
          
          
          
            
              
                
                  Li, Ming,
                
              
            
          
        
          
          
          
            
              
                
                  and Lin, Jimmy
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proceedings of the 36th Conference on Neural Information Processing Systems (NeurIPS 2022).</em>
      
      
        2022
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="https://arxiv.org/pdf/2206.11573.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Most real-world problems that machine learning algorithms are expected to solve
face the situation with 1) unknown data distribution; 2) little domain-specific knowledge; and 3) datasets with limited annotation. We propose Non-Parametric learning
by Compression with Latent Variables (NPC-LV), a learning framework for any
dataset with abundant unlabeled data but very few labeled ones. By only training a
generative model in an unsupervised way, the framework utilizes the data distribution to build a compressor. Using a compressor-based distance metric derived
from Kolmogorov complexity, together with few labeled data, NPC-LV classifies
without further training. We show that NPC-LV outperforms supervised methods
on all three datasets on image classification in low data regime and even outperform
semi-supervised learning methods on CIFAR-10. We demonstrate how and when
negative evidence lowerbound (nELBO) can be used as an approximate compressed
length for classification. By revealing the correlation between compression rate
and classification accuracy, we illustrate that under NPC-LV, the improvement of
generative models can enhance downstream classification accuracy.</p>
    </div>
    
  </div>
</div>
</li></ol>

  <h2 class="year">2021</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">BlackBoxNLP</abbr>
    
  
  </div>

  <div id="jiang-etal-2021-bert" class="col-sm-8">
    
      <div class="title">How Does BERT Rerank Passages? An Attribution Analysis with Information Bottlenecks</div>
      <div class="author">
        
          
          
          
            
              
                
                  Jiang, Zhiying,
                
              
            
          
        
          
          
          
            
              
                
                  Tang, Raphael,
                
              
            
          
        
          
          
          
            
              
                
                  Xin, Ji,
                
              
            
          
        
          
          
          
            
              
                
                  and Lin, Jimmy
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP</em>
      
      
        2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="https://aclanthology.org/2021.blackboxnlp-1.39.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Fine-tuned pre-trained transformers achieve the state of the art in passage reranking. Unfortunately, how they make their predictions remains vastly unexplained, especially at the end-to-end, input-to-output level. Little known is how tokens, layers, and passages precisely contribute to the final prediction. In this paper, we address this gap by leveraging the recently developed information bottlenecks for attribution (IBA) framework. On BERT-based models for passage reranking, we quantitatively demonstrate the framework’s veracity in extracting attribution maps, from which we perform detailed, token-wise analysis about how predictions are made. Overall, we find that BERT still cares about exact token matching for reranking; the [CLS] token mainly gathers information for predictions at the last layer; top-ranked passages are robust to token removal; and BERT fine-tuned on MSMARCO has positional bias towards the start of the passage.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ICLR Workshop</abbr>
    
  
  </div>

  <div id="nogueira2021investigating" class="col-sm-8">
    
      <div class="title">Investigating the Limitations of Transformers with Simple Arithmetic Tasks</div>
      <div class="author">
        
          
          
          
            
              
                
                  Nogueira, Rodrigo,
                
              
            
          
        
          
          
          
            
              
                
                  Jiang, Zhiying,
                
              
            
          
        
          
          
          
            
              
                
                  and Lin, Jimmy
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Proceedings of the First Mathematical Reasoning in General Artificial Intelligence Workshop, ICLR</em>
      
      
        2021
      
      </div>
    

    <div class="links">
    
    
    
    
      
      <a href="https://mathai-iclr.github.io/papers/papers/MATHAI_11_paper.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li></ol>

  <h2 class="year">2020</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">EMNLP2020</abbr>
    
  
  </div>

  <div id="jiang2020inserting" class="col-sm-8">
    
      <div class="title">Inserting Information Bottleneck for Attribution in Transformers</div>
      <div class="author">
        
          
          
          
            
              
                
                  Jiang, Zhiying,
                
              
            
          
        
          
          
          
            
              
                
                  Tang, Raphael,
                
              
            
          
        
          
          
          
            
              
                
                  Xin, Ji,
                
              
            
          
        
          
          
          
            
              
                
                  and Lin, Jimmy
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings</em>
      
      
        2020
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="https://www.aclweb.org/anthology/2020.findings-emnlp.343.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Pretrained transformers achieve the state of the art across tasks in natural language processing, motivating researchers to investigate their inner mechanisms. One common direction is to understand what features are important for prediction. In this paper, we apply information bottlenecks to analyze the attribution of each feature for prediction on a black-box model. We use BERT as the example and evaluate our approach both quantitatively and qualitatively. We show the effectiveness of our method in terms of attribution and the ability to provide insight into how information flows through layers. We demonstrate that our technique outperforms two competitive methods in degradation tests on four datasets. Code is available at https://github.com/bazingagin/IBA.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">EMNLP2020</abbr>
    
  
  </div>

  <div id="nogueira-etal-2020-document" class="col-sm-8">
    
      <div class="title">Document Ranking with a Pretrained Sequence-to-Sequence Model</div>
      <div class="author">
        
          
          
          
            
              
                
                  Nogueira, Rodrigo*,
                
              
            
          
        
          
          
          
            
              
                
                  Jiang, Zhiying*,
                
              
            
          
        
          
          
          
            
              
                
                  Pradeep, Ronak,
                
              
            
          
        
          
          
          
            
              
                
                  and Lin, Jimmy
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings</em>
      
      
        2020
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="https://www.aclweb.org/anthology/2020.findings-emnlp.63.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>This work proposes the use of a pretrained sequence-to-sequence model for document ranking. Our approach is fundamentally different from a commonly adopted classification-based formulation based on encoder-only pretrained transformer architectures such as BERT. We show how a sequence-to-sequence model can be trained to generate relevance labels as “target tokens”, and how the underlying logits of these target tokens can be interpreted as relevance probabilities for ranking. Experimental results on the MS MARCO passage ranking task show that our ranking approach is superior to strong encoder-only models. On three other document retrieval test collections, we demonstrate a zero-shot transfer-based approach that outperforms previous state-of-the-art models requiring in-domain cross-validation. Furthermore, we find that our approach significantly outperforms an encoder-only architecture in a data-poor setting. We investigate this observation in more detail by varying target tokens to probe the model’s use of latent knowledge. Surprisingly, we find that the choice of target tokens impacts effectiveness, even for words that are closely related semantically. This finding sheds some light on why our sequence-to-sequence formulation for document ranking is effective. Code and models are available at pygaggle.ai.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">CEUR</abbr>
    
  
  </div>

  <div id="3d91d2011d1c45e48aa2dbf608c67e5b" class="col-sm-8">
    
      <div class="title">Evaluating pretrained transformer models for citation recommendation</div>
      <div class="author">
        
          
          
          
            
              
                
                  Nogueira, Rodrigo,
                
              
            
          
        
          
          
          
            
              
                
                  Jiang, Zhiying,
                
              
            
          
        
          
          
          
            
              
                
                  Cho, Kyunghyun,
                
              
            
          
        
          
          
          
            
              
                
                  and Lin, Jimmy
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>CEUR Workshop Proceedings</em>
      
      
        2020
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="http://ceur-ws.org/Vol-2591/paper-09.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Citation recommendation systems for the scientific literature, to help authors find papers that should be cited, have the potential to speed up discoveries and uncover new routes for scientific exploration. We treat this task as a ranking problem, which we tackle with a two-stage approach: candidate generation followed by re-ranking. Within this framework, we adapt to the scientific domain a proven combination based on “bag of words” retrieval followed by re-scoring with a BERT model. We experimentally show the effects of domain adaptation, both in terms of pretraining on in-domain data and exploiting in-domain vocabulary. In addition, we evaluate eleven pretrained transformer models and analyze some unexpected failure cases. On three different collections from different scientific disciplines, our models perform close to or at the state of the art in the citation recommendation task.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">arxiv</abbr>
    
  
  </div>

  <div id="nogueira2020navigation" class="col-sm-8">
    
      <div class="title">Navigation-Based Candidate Expansion and Pretrained Language Models for Citation Recommendation</div>
      <div class="author">
        
          
          
          
            
              
                
                  Nogueira, Rodrigo,
                
              
            
          
        
          
          
          
            
              
                
                  Jiang, Zhiying,
                
              
            
          
        
          
          
          
            
              
                
                  Cho, Kyunghyun,
                
              
            
          
        
          
          
          
            
              
                
                  and Lin, Jimmy
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>arXiv preprint arXiv:2001.08687</em>
      
      
        2020
      
      </div>
    

    <div class="links">
    
    
    
    
      
      <a href="https://arxiv.org/pdf/2001.08687.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li></ol>

  <h2 class="year">2019</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ACL2019</abbr>
    
  
  </div>

  <div id="wang2019paperrobot" class="col-sm-8">
    
      <div class="title">PaperRobot: Incremental Draft Generation of Scientific Ideas</div>
      <div class="author">
        
          
          
          
            
              
                
                  Wang, Qingyun,
                
              
            
          
        
          
          
          
            
              
                
                  Huang, Lifu,
                
              
            
          
        
          
          
          
            
              
                
                  Jiang, Zhiying,
                
              
            
          
        
          
          
          
            
              
                
                  Knight, Kevin,
                
              
            
          
        
          
          
          
            
              
                
                  Ji, Heng,
                
              
            
          
        
          
          
          
            
              
                
                  Bansal, Mohit,
                
              
            
          
        
          
          
          
            
              
                
                  and Luan, Yi
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</em>
      
      
        2019
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="https://www.aclweb.org/anthology/P19-1191.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>We present a PaperRobot who performs as an automatic research assistant by (1) conducting deep understanding of a large collection of human-written papers in a target domain and constructing comprehensive background knowledge graphs (KGs); (2) creating new ideas by predicting links from the background KGs, by combining graph attention and contextual text attention; (3) incrementally writing some key elements of a new paper based on memory-attention networks: from the input title along with predicted related entities to generate a paper abstract, from the abstract to generate conclusion and future work, and finally from future work to generate a title for a follow-on paper. Turing Tests, where a biomedical domain expert is asked to compare a system output and a human-authored string, show PaperRobot generated abstracts, conclusion and future work sections, and new titles are chosen over human-written ones up to 30%, 24% and 12% of the time, respectively.</p>
    </div>
    
  </div>
</div>
</li></ol>

  <h2 class="year">2018</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">BEA</abbr>
    
  
  </div>

  <div id="jiang2018chengyu" class="col-sm-8">
    
      <div class="title">Chengyu cloze test</div>
      <div class="author">
        
          
          
          
            
              
                
                  Jiang, Zhiying,
                
              
            
          
        
          
          
          
            
              
                
                  Zhang, Boliang,
                
              
            
          
        
          
          
          
            
              
                
                  Huang, Lifu,
                
              
            
          
        
          
          
          
            
              
                
                  and Ji, Heng
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proceedings of the Thirteenth Workshop on Innovative Use of NLP for Building Educational Applications</em>
      
      
        2018
      
      </div>
    

    <div class="links">
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li></ol>


</div>

  </article>

</div>

    </div>

    <!-- Footer -->

    

  </body>

  <!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>

<!-- Load DarkMode JS -->
<script src="/assets/js/dark_mode.js"></script>


</html>
