<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>Zhiying (Gin)  Jiang</title>
<meta name="description" content="gin's personal website
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://raw.githubusercontent.com/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->
<link rel="shortcut icon" href="/assets/img/favicon.ico">
<link rel="stylesheet" href="/assets/css/main.css">

<link rel="canonical" href="/">
<link rel="preconnect" href="https://fonts.gstatic.com">
<link href="https://fonts.googleapis.com/css2?family=DM+Sans&display=swap" rel="stylesheet">

<!-- Theming-->




    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav sticky-bottom-footer">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <!-- Navbar Toogle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item active">
            <a class="nav-link" href="/">
              about
              
                <span class="sr-only">(current)</span>
              
            </a>
          </li>
          
          <!-- Blog -->
          <li class="nav-item ">
            <a class="nav-link" href="/blog/">
              blog
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/publications/">
                publications
                
              </a>
          </li>
          
          
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    <h1 class="post-title">
     <span class="font-weight-bold">Zhiying (Gin)</span>   Jiang
    </h1>
     <p class="desc"><i>"If the human brain were so simple that we could understand it, we would be so simple that we couldn't."</i></p>
  </header>

  <article>
    
    <div class="profile float-right">
      
        <img class="img-fluid z-depth-1 rounded" src="/assets/img/avatar.png">
      
      
        <div class="address">
          <a href="https://twitter.com/YiqinDai">@Quinn</a> drew this <a href="https://www.klei.com/games/dont-starve">Don't Starve</a> character of me

        </div>
      
    </div>
    

    <div class="clearfix">
      <p><br />
Hi, I’m Gin, a researcher with a passion for understanding and improving both machine learning and human learning.</p>

<p>I’m the co-founder of <a href="https://afaik.io">AFAIK.io</a> (NextAI 2024 cohort). AFAIK is a personalized learning platform that aims at addressing inequalities in access to higher education. Our platform enables anyone to systematically learn anything in-depth at their own level without concerns about hallucinations or misinformation.</p>

<p>Before founding AFAIK, I earned my PhD from the University of Waterloo, where I completed my degree in about 3.5 years under the mentorship of <a href="https://cs.uwaterloo.ca/~jimmylin/">Professor Jimmy Lin</a> and in collaboration with <a href="https://cs.uwaterloo.ca/~mli/">Professor Ming Li</a>. Prior to that, I spent four years at Rensselaer Polytechnic Institute, conducting research at Blender under the guidance of <a href="https://cs.illinois.edu/about/people/faculty/hengji">Professor Heng Ji</a>.</p>

<p>My research focuses on the interpretability and generalizability of machine learning models, with a strong interest in the intersection of information theory and learning. I’m deeply inspired by the idea that compression lies at the heart of both human and machine learning, guiding my exploration of fundamental, theory-driven approaches.</p>

<p>Beyond machine learning, I love neuroscience, physics, and food science.</p>


    </div>

    

    
      <div class="publications">
  <h2>Selected Publications</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ACL2023</abbr>
    
  
  </div>

  <div id="jiang-etal-2023-low" class="col-sm-8">
    
      <div class="title">“Low-Resource” Text Classification: A Parameter-Free Classification Method with Compressors</div>
      <div class="author">
        
          
          
          
            
              
                
                  Jiang, Zhiying,
                
              
            
          
        
          
          
          
            
              
                
                  Yang, Matthew,
                
              
            
          
        
          
          
          
            
              
                
                  Tsirlin, Mikhail,
                
              
            
          
        
          
          
          
            
              
                
                  Tang, Raphael,
                
              
            
          
        
          
          
          
            
              
                
                  Dai, Yiqin,
                
              
            
          
        
          
          
          
            
              
                
                  and Lin, Jimmy
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Findings of the Association for Computational Linguistics (ACL)</em>
      
      
        2023
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="https://aclanthology.org/2023.findings-acl.426/" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Deep neural networks (DNNs) are often used for text classification due to their high accuracy. However, DNNs can be computationally intensive, requiring millions of parameters and large amounts of labeled data, which can make them expensive to use, to optimize, and to transfer to out-of-distribution (OOD) cases in practice. In this paper, we propose a non-parametric alternative to DNNs that’s easy, lightweight, and universal in text classification: a combination of a simple compressor like gzip with a k-nearest-neighbor classifier. Without any training parameters, our method achieves results that are competitive with non-pretrained deep learning methods on six in-distribution datasets.It even outperforms BERT on all five OOD datasets, including four low-resource languages. Our method also excels in the few-shot setting, where labeled data are too scarce to train DNNs effectively. Our code is available at https://github.com/bazingagin/npc_gzip.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ACL2023</abbr>
    
  
  </div>

  <div id="tang-etal-2023-daam" class="col-sm-8">
    
      <div class="title">What the DAAM: Interpreting Stable Diffusion Using Cross Attention</div>
      <div class="author">
        
          
          
          
            
              
                
                  Tang, Raphael,
                
              
            
          
        
          
          
          
            
              
                
                  Liu, Linqing,
                
              
            
          
        
          
          
          
            
              
                
                  Pandey, Akshat,
                
              
            
          
        
          
          
          
            
              
                
                  Jiang, Zhiying,
                
              
            
          
        
          
          
          
            
              
                
                  Yang, Gefei,
                
              
            
          
        
          
          
          
            
              
                
                  Kumar, Karun,
                
              
            
          
        
          
          
          
            
              
                
                  Stenetorp, Pontus,
                
              
            
          
        
          
          
          
            
              
                
                  Lin, Jimmy,
                
              
            
          
        
          
          
          
            
              
                
                  and Ture, Ferhan
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proceedings of Association for Computational Linguistics (ACL), Best Paper Award,</em>
      
      
        2023
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Diffusion models are a milestone in text-to-image generation, but they remain poorly understood, lacking interpretability analyses. In this paper, we perform a text-image attribution analysis on Stable Diffusion, a recently open-sourced model. To produce attribution maps, we upscale and aggregate cross-attention maps in the denoising module, naming our method DAAM. We validate it by testing its segmentation ability on nouns, as well as its generalized attribution quality on all parts of speech, rated by humans. On two generated datasets, we attain a competitive 58.8-64.8 mIoU on noun segmentation and fair to good mean opinion scores (3.4-4.2) on generalized attribution. Then, we apply DAAM to study the role of syntax in the pixel space across head–dependent heat map interaction patterns for ten common dependency relations. We show that, for some relations, the head map consistently subsumes the dependent, while the opposite is true for others. Finally, we study several semantic phenomena, focusing on feature entanglement; we find that the presence of cohyponyms worsens generation quality by 9%, and descriptive adjectives attend too broadly. We are the first to interpret large diffusion models from a visuolinguistic perspective, which enables future research. Our code is at https://github.com/castorini/daam.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">NeurIPS2022</abbr>
    
  
  </div>

  <div id="jiang2022few" class="col-sm-8">
    
      <div class="title">Few-Shot Non-Parametric Learning with Deep Latent Variable Model</div>
      <div class="author">
        
          
          
          
            
              
                
                  Jiang, Zhiying,
                
              
            
          
        
          
          
          
            
              
                
                  Dai, Yiqin,
                
              
            
          
        
          
          
          
            
              
                
                  Xin, Ji,
                
              
            
          
        
          
          
          
            
              
                
                  Li, Ming,
                
              
            
          
        
          
          
          
            
              
                
                  and Lin, Jimmy
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proceedings of the 36th Conference on Neural Information Processing Systems (NeurIPS) Spotlight.</em>
      
      
        2022
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="https://arxiv.org/pdf/2206.11573.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Most real-world problems that machine learning algorithms are expected to solve
face the situation with 1) unknown data distribution; 2) little domain-specific knowledge; and 3) datasets with limited annotation. We propose Non-Parametric learning
by Compression with Latent Variables (NPC-LV), a learning framework for any
dataset with abundant unlabeled data but very few labeled ones. By only training a
generative model in an unsupervised way, the framework utilizes the data distribution to build a compressor. Using a compressor-based distance metric derived
from Kolmogorov complexity, together with few labeled data, NPC-LV classifies
without further training. We show that NPC-LV outperforms supervised methods
on all three datasets on image classification in low data regime and even outperform
semi-supervised learning methods on CIFAR-10. We demonstrate how and when
negative evidence lowerbound (nELBO) can be used as an approximate compressed
length for classification. By revealing the correlation between compression rate
and classification accuracy, we illustrate that under NPC-LV, the improvement of
generative models can enhance downstream classification accuracy.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">BlackBoxNLP</abbr>
    
  
  </div>

  <div id="jiang-etal-2021-bert" class="col-sm-8">
    
      <div class="title">How Does BERT Rerank Passages? An Attribution Analysis with Information Bottlenecks</div>
      <div class="author">
        
          
          
          
            
              
                
                  Jiang, Zhiying,
                
              
            
          
        
          
          
          
            
              
                
                  Tang, Raphael,
                
              
            
          
        
          
          
          
            
              
                
                  Xin, Ji,
                
              
            
          
        
          
          
          
            
              
                
                  and Lin, Jimmy
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP</em>
      
      
        2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="https://aclanthology.org/2021.blackboxnlp-1.39.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Fine-tuned pre-trained transformers achieve the state of the art in passage reranking. Unfortunately, how they make their predictions remains vastly unexplained, especially at the end-to-end, input-to-output level. Little known is how tokens, layers, and passages precisely contribute to the final prediction. In this paper, we address this gap by leveraging the recently developed information bottlenecks for attribution (IBA) framework. On BERT-based models for passage reranking, we quantitatively demonstrate the framework’s veracity in extracting attribution maps, from which we perform detailed, token-wise analysis about how predictions are made. Overall, we find that BERT still cares about exact token matching for reranking; the [CLS] token mainly gathers information for predictions at the last layer; top-ranked passages are robust to token removal; and BERT fine-tuned on MSMARCO has positional bias towards the start of the passage.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">EMNLP2020</abbr>
    
  
  </div>

  <div id="jiang2020inserting" class="col-sm-8">
    
      <div class="title">Inserting Information Bottleneck for Attribution in Transformers</div>
      <div class="author">
        
          
          
          
            
              
                
                  Jiang, Zhiying,
                
              
            
          
        
          
          
          
            
              
                
                  Tang, Raphael,
                
              
            
          
        
          
          
          
            
              
                
                  Xin, Ji,
                
              
            
          
        
          
          
          
            
              
                
                  and Lin, Jimmy
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings</em>
      
      
        2020
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="https://www.aclweb.org/anthology/2020.findings-emnlp.343.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Pretrained transformers achieve the state of the art across tasks in natural language processing, motivating researchers to investigate their inner mechanisms. One common direction is to understand what features are important for prediction. In this paper, we apply information bottlenecks to analyze the attribution of each feature for prediction on a black-box model. We use BERT as the example and evaluate our approach both quantitatively and qualitatively. We show the effectiveness of our method in terms of attribution and the ability to provide insight into how information flows through layers. We demonstrate that our technique outperforms two competitive methods in degradation tests on four datasets. Code is available at https://github.com/bazingagin/IBA.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">EMNLP2020</abbr>
    
  
  </div>

  <div id="nogueira-etal-2020-document" class="col-sm-8">
    
      <div class="title">Document Ranking with a Pretrained Sequence-to-Sequence Model</div>
      <div class="author">
        
          
          
          
            
              
                
                  Nogueira, Rodrigo*,
                
              
            
          
        
          
          
          
            
              
                
                  Jiang, Zhiying*,
                
              
            
          
        
          
          
          
            
              
                
                  Pradeep, Ronak,
                
              
            
          
        
          
          
          
            
              
                
                  and Lin, Jimmy
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings</em>
      
      
        2020
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="https://www.aclweb.org/anthology/2020.findings-emnlp.63.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>This work proposes the use of a pretrained sequence-to-sequence model for document ranking. Our approach is fundamentally different from a commonly adopted classification-based formulation based on encoder-only pretrained transformer architectures such as BERT. We show how a sequence-to-sequence model can be trained to generate relevance labels as “target tokens”, and how the underlying logits of these target tokens can be interpreted as relevance probabilities for ranking. Experimental results on the MS MARCO passage ranking task show that our ranking approach is superior to strong encoder-only models. On three other document retrieval test collections, we demonstrate a zero-shot transfer-based approach that outperforms previous state-of-the-art models requiring in-domain cross-validation. Furthermore, we find that our approach significantly outperforms an encoder-only architecture in a data-poor setting. We investigate this observation in more detail by varying target tokens to probe the model’s use of latent knowledge. Surprisingly, we find that the choice of target tokens impacts effectiveness, even for words that are closely related semantically. This finding sheds some light on why our sequence-to-sequence formulation for document ranking is effective. Code and models are available at pygaggle.ai.</p>
    </div>
    
  </div>
</div>
</li></ol>
</div>

    

    
    <div class="social">
      <span class="contact-icon text-center">
  <a href="mailto:%62%61%7A%69%6E%67%61%67%69%6E@%67%6D%61%69%6C.%63%6F%6D"><i class="fas fa-envelope"></i></a>
  
  <a href="https://scholar.google.com/citations?user=eJ5MnJ8AAAAJ" target="_blank" title="Google Scholar"><i class="ai ai-google-scholar"></i></a>
  
  
  <a href="https://github.com/bazingagin" target="_blank" title="GitHub"><i class="fab fa-github"></i></a>
  
  <a href="https://twitter.com/ZhiyingJ" target="_blank" title="Twitter"><i class="fab fa-twitter"></i></a>
  
  
  
  
  
</span>

      <div class="contact-note"></div>
    </div>
    
  </article>

</div>

    </div>

    <!-- Footer -->

    

  </body>

  <!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>

<!-- Load DarkMode JS -->
<script src="/assets/js/dark_mode.js"></script>


</html>
