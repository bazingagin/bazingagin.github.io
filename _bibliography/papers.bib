---
---

@inproceedings{jiang2020inserting,
  abbr={EMNLP2020},
  title={Inserting Information Bottleneck for Attribution in Transformers},
  author={Jiang, Zhiying and Tang, Raphael and Xin, Ji and Lin, Jimmy},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings},
  pages={3850--3857},
  pdf={https://www.aclweb.org/anthology/2020.findings-emnlp.343.pdf},
      abstract = "Pretrained transformers achieve the state of the art across tasks in natural language processing, motivating researchers to investigate their inner mechanisms. One common direction is to understand what features are important for prediction. In this paper, we apply information bottlenecks to analyze the attribution of each feature for prediction on a black-box model. We use BERT as the example and evaluate our approach both quantitatively and qualitatively. We show the effectiveness of our method in terms of attribution and the ability to provide insight into how information flows through layers. We demonstrate that our technique outperforms two competitive methods in degradation tests on four datasets. Code is available at https://github.com/bazingagin/IBA.",
  year={2020},
  selected={true}
}


@inproceedings{nogueira-etal-2020-document,
    title = "Document Ranking with a Pretrained Sequence-to-Sequence Model",
    author = "Nogueira, Rodrigo  and
      Jiang, Zhiying  and
      Pradeep, Ronak  and
      Lin, Jimmy",
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings},
    month = nov,
    abbr={EMNLP2020},
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.findings-emnlp.63",
    doi = "10.18653/v1/2020.findings-emnlp.63",
    pages = "708--718",
    abstract = "This work proposes the use of a pretrained sequence-to-sequence model for document ranking. Our approach is fundamentally different from a commonly adopted classification-based formulation based on encoder-only pretrained transformer architectures such as BERT. We show how a sequence-to-sequence model can be trained to generate relevance labels as {``}target tokens{''}, and how the underlying logits of these target tokens can be interpreted as relevance probabilities for ranking. Experimental results on the MS MARCO passage ranking task show that our ranking approach is superior to strong encoder-only models. On three other document retrieval test collections, we demonstrate a zero-shot transfer-based approach that outperforms previous state-of-the-art models requiring in-domain cross-validation. Furthermore, we find that our approach significantly outperforms an encoder-only architecture in a data-poor setting. We investigate this observation in more detail by varying target tokens to probe the model{'}s use of latent knowledge. Surprisingly, we find that the choice of target tokens impacts effectiveness, even for words that are closely related semantically. This finding sheds some light on why our sequence-to-sequence formulation for document ranking is effective. Code and models are available at pygaggle.ai.",
    selected={true},
    pdf={https://www.aclweb.org/anthology/2020.findings-emnlp.63.pdf}
}

@inproceedings{wang2019paperrobot,
  abbr = {ACL2019},
  title={PaperRobot: Incremental Draft Generation of Scientific Ideas},
  author={Wang, Qingyun and Huang, Lifu and Jiang, Zhiying and Knight, Kevin and Ji, Heng and Bansal, Mohit and Luan, Yi},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={1980--1991},
  year={2019},
  selected={true},
  pdf={https://www.aclweb.org/anthology/P19-1191.pdf},
  abstract = "We present a PaperRobot who performs as an automatic research assistant by (1) conducting deep understanding of a large collection of human-written papers in a target domain and constructing comprehensive background knowledge graphs (KGs); (2) creating new ideas by predicting links from the background KGs, by combining graph attention and contextual text attention; (3) incrementally writing some key elements of a new paper based on memory-attention networks: from the input title along with predicted related entities to generate a paper abstract, from the abstract to generate conclusion and future work, and finally from future work to generate a title for a follow-on paper. Turing Tests, where a biomedical domain expert is asked to compare a system output and a human-authored string, show PaperRobot generated abstracts, conclusion and future work sections, and new titles are chosen over human-written ones up to 30{\%}, 24{\%} and 12{\%} of the time, respectively."
}

@inproceedings{jiang2018chengyu,
  abbr = {BEA},
  title={Chengyu cloze test},
  author={Jiang, Zhiying and Zhang, Boliang and Huang, Lifu and Ji, Heng},
  booktitle={Proceedings of the Thirteenth Workshop on Innovative Use of NLP for Building Educational Applications},
  pages={154--158},
  year={2018},
  selected={true}
}

@article{3d91d2011d1c45e48aa2dbf608c67e5b,
title = "Evaluating pretrained transformer models for citation recommendation",
abstract = "Citation recommendation systems for the scientific literature, to help authors find papers that should be cited, have the potential to speed up discoveries and uncover new routes for scientific exploration. We treat this task as a ranking problem, which we tackle with a two-stage approach: candidate generation followed by re-ranking. Within this framework, we adapt to the scientific domain a proven combination based on “bag of words” retrieval followed by re-scoring with a BERT model. We experimentally show the effects of domain adaptation, both in terms of pretraining on in-domain data and exploiting in-domain vocabulary. In addition, we evaluate eleven pretrained transformer models and analyze some unexpected failure cases. On three different collections from different scientific disciplines, our models perform close to or at the state of the art in the citation recommendation task.",
author = "Rodrigo Nogueira and Zhiying Jiang and Kyunghyun Cho and Jimmy Lin",
year = "2020",
language = "English (US)",
volume = "2591",
pages = "89--100",
journal = "CEUR Workshop Proceedings",
issn = "1613-0073",
publisher = "CEUR-WS",
note = "10th International Workshop on Bibliometric-Enhanced Information Retrieval, BIR 2020 ; Conference date: 14-04-2020",
selected={true},
abbr={CEUR},
pdf={http://ceur-ws.org/Vol-2591/paper-09.pdf},
}

@article{nogueira2020navigation,
  title={Navigation-Based Candidate Expansion and Pretrained Language Models for Citation Recommendation},
  author={Nogueira, Rodrigo and Jiang, Zhiying and Cho, Kyunghyun and Lin, Jimmy},
  journal={arXiv preprint arXiv:2001.08687},
  year={2020},
  pdf={https://arxiv.org/pdf/2001.08687.pdf},
  abbr={arxiv}
}



