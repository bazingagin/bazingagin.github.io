---
---
@inproceedings{jiang-etal-2023-low,
    abbr={ACL2023},
    title = "{``}Low-Resource{''} Text Classification: A Parameter-Free Classification Method with Compressors",
    author = "Jiang, Zhiying  and
      Yang, Matthew  and
      Tsirlin, Mikhail  and
      Tang, Raphael  and
      Dai, Yiqin  and
      Lin, Jimmy",
    booktitle = "Findings of the Association for Computational Linguistics (ACL)",
    pdf={https://aclanthology.org/2023.findings-acl.426/},
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-acl.426",
    doi = "10.18653/v1/2023.findings-acl.426",
    pages = "6810--6828",
    abstract = "Deep neural networks (DNNs) are often used for text classification due to their high accuracy. However, DNNs can be computationally intensive, requiring millions of parameters and large amounts of labeled data, which can make them expensive to use, to optimize, and to transfer to out-of-distribution (OOD) cases in practice. In this paper, we propose a non-parametric alternative to DNNs that{'}s easy, lightweight, and universal in text classification: a combination of a simple compressor like \textit{gzip} with a $k$-nearest-neighbor classifier. Without any training parameters, our method achieves results that are competitive with non-pretrained deep learning methods on six in-distribution datasets.It even outperforms BERT on all five OOD datasets, including four low-resource languages. Our method also excels in the few-shot setting, where labeled data are too scarce to train DNNs effectively.",
    selected={true},
}

@inproceedings{huang-2023-approx,
    abbr={preprint},
    title = "Approximating Human-Like Few-shot Learning with GPT-based Compression",
    author = "Huang, Cynthia*  and
      Xie, Yuqing*  and
      Jiang, Zhiying*  and
      Lin, Jimmy  and
      Li, Ming",
    booktitle = "preprint",
    pdf={https://arxiv.org/abs/2308.06942},
    month = aug,
    year = "2023",
    url = "https://arxiv.org/abs/2308.06942",
    abstract = "In this work, we conceptualize the learning process as information compression. We seek to equip generative pre-trained models with human-like learning capabilities that enable data compression during inference. We present a novel approach that utilizes the Generative Pre-trained Transformer (GPT) to approximate Kolmogorov complexity, with the aim of estimating the optimal Information Distance for few-shot learning. We first propose using GPT as a prior for lossless text compression, achieving a noteworthy compression ratio. Experiment with LLAMA2-7B backbone achieves a compression ratio of 15.5 on enwik9. We justify the pre-training objective of GPT models by demonstrating its equivalence to the compression length, and, consequently, its ability to approximate the information distance for texts. Leveraging the approximated information distance, our method allows the direct application of GPT models in quantitative text similarity measurements. Experiment results show that our method overall achieves superior performance compared to embedding and prompt baselines on challenging NLP tasks, including semantic similarity, zero and one-shot text classification, and zero-shot text ranking.",
    selected={false},
}

@inproceedings{tang-etal-2023-daam,
abbr={ACL2023},
    title = "What the {DAAM}: Interpreting Stable Diffusion Using Cross Attention",
    author = "Tang, Raphael  and
      Liu, Linqing  and
      Pandey, Akshat  and
      Jiang, Zhiying  and
      Yang, Gefei  and
      Kumar, Karun  and
      Stenetorp, Pontus  and
      Lin, Jimmy  and
      Ture, Ferhan",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of Association for Computational Linguistics (ACL), Best Paper Award,",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.310",
    doi = "10.18653/v1/2023.acl-long.310",
    pages = "5644--5659",
    abstract = "Diffusion models are a milestone in text-to-image generation, but they remain poorly understood, lacking interpretability analyses. In this paper, we perform a text-image attribution analysis on Stable Diffusion, a recently open-sourced model. To produce attribution maps, we upscale and aggregate cross-attention maps in the denoising module, naming our method DAAM. We validate it by testing its segmentation ability on nouns, as well as its generalized attribution quality on all parts of speech, rated by humans. On two generated datasets, we attain a competitive 58.8-64.8 mIoU on noun segmentation and fair to good mean opinion scores (3.4-4.2) on generalized attribution. Then, we apply DAAM to study the role of syntax in the pixel space across head{--}dependent heat map interaction patterns for ten common dependency relations. We show that, for some relations, the head map consistently subsumes the dependent, while the opposite is true for others. Finally, we study several semantic phenomena, focusing on feature entanglement; we find that the presence of cohyponyms worsens generation quality by 9{\%}, and descriptive adjectives attend too broadly. We are the first to interpret large diffusion models from a visuolinguistic perspective, which enables future research. Our code is at \url{https://github.com/castorini/daam}.",
    selected={true},
}


@inproceedings{jiang2022few,
  abbr={NeurIPS2022},
  title={Few-Shot Non-Parametric Learning with Deep Latent Variable Model},
  author={Jiang, Zhiying and Dai, Yiqin and Xin, Ji and Li, Ming and Lin, Jimmy},
  journal={arXiv preprint arXiv:2206.11573},
  booktitle={Proceedings of the 36th Conference on Neural Information Processing Systems (NeurIPS) Spotlight.},
  pdf={https://arxiv.org/pdf/2206.11573.pdf},
  abstract = "Most real-world problems that machine learning algorithms are expected to solve
face the situation with 1) unknown data distribution; 2) little domain-specific knowledge; and 3) datasets with limited annotation. We propose Non-Parametric learning
by Compression with Latent Variables (NPC-LV), a learning framework for any
dataset with abundant unlabeled data but very few labeled ones. By only training a
generative model in an unsupervised way, the framework utilizes the data distribution to build a compressor. Using a compressor-based distance metric derived
from Kolmogorov complexity, together with few labeled data, NPC-LV classifies
without further training. We show that NPC-LV outperforms supervised methods
on all three datasets on image classification in low data regime and even outperform
semi-supervised learning methods on CIFAR-10. We demonstrate how and when
negative evidence lowerbound (nELBO) can be used as an approximate compressed
length for classification. By revealing the correlation between compression rate
and classification accuracy, we illustrate that under NPC-LV, the improvement of
generative models can enhance downstream classification accuracy.",
  year={2022},
  selected={true}
}

@inproceedings{jiang-etal-2021-bert,
    abbr={BlackBoxNLP},
    title = "How Does {BERT} Rerank Passages? An Attribution Analysis with Information Bottlenecks",
    author = "Jiang, Zhiying  and
      Tang, Raphael  and
      Xin, Ji  and
      Lin, Jimmy",
    booktitle = "Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP",
    month = nov,
    year = "2021",
    address = "Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.blackboxnlp-1.39",
    pages = "496--509",
    pdf={https://aclanthology.org/2021.blackboxnlp-1.39.pdf},
    abstract = "Fine-tuned pre-trained transformers achieve the state of the art in passage reranking. Unfortunately, how they make their predictions remains vastly unexplained, especially at the end-to-end, input-to-output level. Little known is how tokens, layers, and passages precisely contribute to the final prediction. In this paper, we address this gap by leveraging the recently developed information bottlenecks for attribution (IBA) framework. On BERT-based models for passage reranking, we quantitatively demonstrate the framework{'}s veracity in extracting attribution maps, from which we perform detailed, token-wise analysis about how predictions are made. Overall, we find that BERT still cares about exact token matching for reranking; the [CLS] token mainly gathers information for predictions at the last layer; top-ranked passages are robust to token removal; and BERT fine-tuned on MSMARCO has positional bias towards the start of the passage.",
    selected={true}
}

@article{nogueira2021investigating,
  abbr={ICLR Workshop},
  title={Investigating the Limitations of Transformers with Simple Arithmetic Tasks},
  author={Nogueira, Rodrigo and Jiang, Zhiying and Lin, Jimmy},
  url="https://mathai-iclr.github.io/papers/papers/MATHAI_11_paper.pdf",
  pdf={https://mathai-iclr.github.io/papers/papers/MATHAI_11_paper.pdf},
  journal={Proceedings of the First Mathematical Reasoning in General Artificial Intelligence Workshop, ICLR},
  year={2021}
}

@inproceedings{jiang2020inserting,
  abbr={EMNLP2020},
  title={Inserting Information Bottleneck for Attribution in Transformers},
  author={Jiang, Zhiying and Tang, Raphael and Xin, Ji and Lin, Jimmy},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings},
  pages={3850--3857},
  pdf={https://www.aclweb.org/anthology/2020.findings-emnlp.343.pdf},
      abstract = "Pretrained transformers achieve the state of the art across tasks in natural language processing, motivating researchers to investigate their inner mechanisms. One common direction is to understand what features are important for prediction. In this paper, we apply information bottlenecks to analyze the attribution of each feature for prediction on a black-box model. We use BERT as the example and evaluate our approach both quantitatively and qualitatively. We show the effectiveness of our method in terms of attribution and the ability to provide insight into how information flows through layers. We demonstrate that our technique outperforms two competitive methods in degradation tests on four datasets. Code is available at https://github.com/bazingagin/IBA.",
  year={2020},
  selected={true}
}


@inproceedings{nogueira-etal-2020-document,
    title = "Document Ranking with a Pretrained Sequence-to-Sequence Model",
    author = "Nogueira, Rodrigo*  and
      Jiang, Zhiying*  and
      Pradeep, Ronak  and
      Lin, Jimmy",
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings},
    month = nov,
    abbr={EMNLP2020},
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.findings-emnlp.63",
    doi = "10.18653/v1/2020.findings-emnlp.63",
    pages = "708--718",
    abstract = "This work proposes the use of a pretrained sequence-to-sequence model for document ranking. Our approach is fundamentally different from a commonly adopted classification-based formulation based on encoder-only pretrained transformer architectures such as BERT. We show how a sequence-to-sequence model can be trained to generate relevance labels as {``}target tokens{''}, and how the underlying logits of these target tokens can be interpreted as relevance probabilities for ranking. Experimental results on the MS MARCO passage ranking task show that our ranking approach is superior to strong encoder-only models. On three other document retrieval test collections, we demonstrate a zero-shot transfer-based approach that outperforms previous state-of-the-art models requiring in-domain cross-validation. Furthermore, we find that our approach significantly outperforms an encoder-only architecture in a data-poor setting. We investigate this observation in more detail by varying target tokens to probe the model{'}s use of latent knowledge. Surprisingly, we find that the choice of target tokens impacts effectiveness, even for words that are closely related semantically. This finding sheds some light on why our sequence-to-sequence formulation for document ranking is effective. Code and models are available at pygaggle.ai.",
    selected={true},
    pdf={https://www.aclweb.org/anthology/2020.findings-emnlp.63.pdf}
}

@inproceedings{wang2019paperrobot,
  abbr = {ACL2019},
  title={PaperRobot: Incremental Draft Generation of Scientific Ideas},
  author={Wang, Qingyun and Huang, Lifu and Jiang, Zhiying and Knight, Kevin and Ji, Heng and Bansal, Mohit and Luan, Yi},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={1980--1991},
  year={2019},
  pdf={https://www.aclweb.org/anthology/P19-1191.pdf},
  abstract = "We present a PaperRobot who performs as an automatic research assistant by (1) conducting deep understanding of a large collection of human-written papers in a target domain and constructing comprehensive background knowledge graphs (KGs); (2) creating new ideas by predicting links from the background KGs, by combining graph attention and contextual text attention; (3) incrementally writing some key elements of a new paper based on memory-attention networks: from the input title along with predicted related entities to generate a paper abstract, from the abstract to generate conclusion and future work, and finally from future work to generate a title for a follow-on paper. Turing Tests, where a biomedical domain expert is asked to compare a system output and a human-authored string, show PaperRobot generated abstracts, conclusion and future work sections, and new titles are chosen over human-written ones up to 30{\%}, 24{\%} and 12{\%} of the time, respectively."
}

@inproceedings{jiang2018chengyu,
  abbr = {BEA},
  title={Chengyu cloze test},
  author={Jiang, Zhiying and Zhang, Boliang and Huang, Lifu and Ji, Heng},
  booktitle={Proceedings of the Thirteenth Workshop on Innovative Use of NLP for Building Educational Applications},
  pages={154--158},
  year={2018}
}

@article{3d91d2011d1c45e48aa2dbf608c67e5b,
title = "Evaluating pretrained transformer models for citation recommendation",
abstract = "Citation recommendation systems for the scientific literature, to help authors find papers that should be cited, have the potential to speed up discoveries and uncover new routes for scientific exploration. We treat this task as a ranking problem, which we tackle with a two-stage approach: candidate generation followed by re-ranking. Within this framework, we adapt to the scientific domain a proven combination based on “bag of words” retrieval followed by re-scoring with a BERT model. We experimentally show the effects of domain adaptation, both in terms of pretraining on in-domain data and exploiting in-domain vocabulary. In addition, we evaluate eleven pretrained transformer models and analyze some unexpected failure cases. On three different collections from different scientific disciplines, our models perform close to or at the state of the art in the citation recommendation task.",
author = "Rodrigo Nogueira and Zhiying Jiang and Kyunghyun Cho and Jimmy Lin",
year = "2020",
language = "English (US)",
volume = "2591",
pages = "89--100",
journal = "CEUR Workshop Proceedings",
issn = "1613-0073",
publisher = "CEUR-WS",
note = "10th International Workshop on Bibliometric-Enhanced Information Retrieval, BIR 2020 ; Conference date: 14-04-2020",
abbr={CEUR},
pdf={http://ceur-ws.org/Vol-2591/paper-09.pdf},
}

@article{nogueira2020navigation,
  title={Navigation-Based Candidate Expansion and Pretrained Language Models for Citation Recommendation},
  author={Nogueira, Rodrigo and Jiang, Zhiying and Cho, Kyunghyun and Lin, Jimmy},
  journal={arXiv preprint arXiv:2001.08687},
  year={2020},
  pdf={https://arxiv.org/pdf/2001.08687.pdf},
  abbr={arxiv}
}



